{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Masking\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# map district name to district number\n",
    "district_mapping_path = \"../airflow/dags/data/districts.xlsx\"\n",
    "\n",
    "district_mapping = pd.read_excel(district_mapping_path)\n",
    "\n",
    "\n",
    "def get_district_name(district_no):\n",
    "    return district_mapping[district_mapping[\"Postal District\"] == district_no][\n",
    "        \"General Location\"\n",
    "    ].values[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Previous Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all previous files\n",
    "# open pickle\n",
    "train_df_dict = pd.read_pickle('data/train_df_dict.pkl')\n",
    "train_df_dict_L = pd.read_pickle('data/train_df_dict_L.pkl')\n",
    "test_df_dict = pd.read_pickle('data/test_df_dict.pkl')\n",
    "\n",
    "# import resale_flat_transactions_clean\n",
    "resale_flat_transactions_clean = pd.read_csv('data/resale_flat_transactions_clean.csv')\n",
    "\n",
    "# import features \n",
    "all_district_var_ts = pd.read_pickle('data/all_district_var_ts.pkl')\n",
    "\n",
    "# replace NaN values with 0 since we are using LSTM\n",
    "# Mask layer will mask the 0 values\n",
    "# merge the train set with the features\n",
    "train_lstm_df_dict = {}\n",
    "for district_no, district_df in train_df_dict.items():\n",
    "    train_lstm_df_dict[district_no] = district_df.to_frame().merge(all_district_var_ts[district_no], left_index=True, right_index=True, how='left')\n",
    "    # replace the NaN values with 0\n",
    "    train_lstm_df_dict[district_no] = train_lstm_df_dict[district_no].fillna(0)\n",
    "\n",
    "train_lstm_df_dict_L = {}\n",
    "for district_no, district_df in train_df_dict_L.items():\n",
    "    train_lstm_df_dict_L[district_no] = district_df.to_frame().merge(all_district_var_ts[district_no], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# merge the test set with the features\n",
    "test_lstm_df_dict = {}\n",
    "for district_no, district_df in test_df_dict.items():\n",
    "    test_lstm_df_dict[district_no] = district_df.to_frame().merge(all_district_var_ts[district_no], left_index=True, right_index=True, how='left')\n",
    "    # replace the NaN values with 0\n",
    "    test_lstm_df_dict[district_no] = test_lstm_df_dict[district_no].fillna(0)\n",
    "\n",
    "# Normalize the data\n",
    "# create scalers for each district\n",
    "scaler_dict = {}\n",
    "for district_no, district_df in train_lstm_df_dict.items():\n",
    "    scaler_dict[district_no] = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_dict[district_no].fit(district_df)\n",
    "\n",
    "# create scalers for each district\n",
    "scaler_dict_L = {}\n",
    "for district_no, district_df in train_lstm_df_dict_L.items():\n",
    "    scaler_dict_L[district_no] = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_dict_L[district_no].fit(district_df)\n",
    "\n",
    "# transform the train set\n",
    "train_lstm_df_dict_scaled = {}\n",
    "for district_no, district_df in train_lstm_df_dict.items():\n",
    "    train_lstm_df_dict_scaled[district_no] = pd.DataFrame(\n",
    "        scaler_dict[district_no].transform(district_df),\n",
    "        columns=district_df.columns,\n",
    "        index=district_df.index,\n",
    "    )\n",
    "\n",
    "# transform the train set\n",
    "train_lstm_df_dict_scaled_L = {}\n",
    "for district_no, district_df in train_lstm_df_dict_L.items():\n",
    "    train_lstm_df_dict_scaled_L[district_no] = pd.DataFrame(\n",
    "        scaler_dict_L[district_no].transform(district_df),\n",
    "        columns=district_df.columns,\n",
    "        index=district_df.index,\n",
    "    )\n",
    "\n",
    "# transform the test set\n",
    "test_lstm_df_dict_scaled = {}\n",
    "for district_no, district_df in test_lstm_df_dict.items():\n",
    "    test_lstm_df_dict_scaled[district_no] = pd.DataFrame(\n",
    "        scaler_dict[district_no].transform(district_df),\n",
    "        columns=district_df.columns,\n",
    "        index=district_df.index,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all data \n",
    "\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "lookback = 3\n",
    "num_features = len(train_lstm_df_dict_scaled[1].columns)\n",
    "\n",
    "# create dataset for each district\n",
    "train_lstm_dataset_dict = {}\n",
    "for district_no, district_df in train_lstm_df_dict_scaled.items():\n",
    "    train_lstm_dataset_dict[district_no] = TimeseriesGenerator(district_df.values, district_df.values, length=lookback, batch_size=1)\n",
    "\n",
    "# create dataset for each district\n",
    "train_lstm_dataset_dict_L = {}\n",
    "for district_no, district_df in train_lstm_df_dict_scaled_L.items():\n",
    "    train_lstm_dataset_dict_L[district_no] = TimeseriesGenerator(district_df.values, district_df.values, length=lookback, batch_size=1)\n",
    "\n",
    "# create dataset for each district\n",
    "test_lstm_dataset_dict = {}\n",
    "for district_no, district_df in test_lstm_df_dict_scaled.items():\n",
    "    test_lstm_dataset_dict[district_no] = TimeseriesGenerator(district_df.values, district_df.values, length=lookback, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model on Keras example on district 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_56498/1683472848.py:16: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n",
      "2023-04-02 22:59:05.160987: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 3s 48ms/step - loss: 0.6161\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 2s 43ms/step - loss: 0.5506\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 0.5485\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 0.5501\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 2s 45ms/step - loss: 0.5483\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 0.5433\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 0.5468\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 0.5459\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 0.5415\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 2s 43ms/step - loss: 0.5446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d7410580>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Masking(\n",
    "        mask_value=0.,\n",
    "        input_shape=(\n",
    "            lookback,\n",
    "            num_features,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(units=50, activation=\"relu\"))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "model.fit_generator(\n",
    "    train_lstm_dataset_dict[1],\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 10ms/step\n",
      "(21, 1)\n",
      "(24, 9)\n",
      "(24, 9)\n"
     ]
    }
   ],
   "source": [
    "# get the predictions\n",
    "predictions = model.predict(test_lstm_dataset_dict[1])\n",
    "\n",
    "print(predictions.shape) # number of predictions is 3 less than the number of test data\n",
    "\n",
    "# pad the predictions with 0 \n",
    "predictions_padded = np.zeros((test_lstm_df_dict_scaled[1].shape[0], num_features)) # create array of zeros with the same shape as the test data\n",
    "print(predictions_padded.shape)\n",
    "predictions_padded[lookback:, 0] = predictions[:, 0] # fill the first column with the predictions\n",
    "print(predictions_padded.shape)\n",
    "# inverse transform the predictions\n",
    "predictions_inversed = scaler_dict[1].inverse_transform(predictions_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 9)\n",
      "(24, 9)\n"
     ]
    }
   ],
   "source": [
    "print(test_lstm_df_dict_scaled[1].shape)\n",
    "print(predictions_inversed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm(model, train_set, test_set):\n",
    "    # make predictions\n",
    "    train_predict = model.predict(train_set)\n",
    "    print(train_predict.shape)\n",
    "    test_predict = model.predict(test_set)\n",
    "    # pad the predictions\n",
    "    train_predict_padded = np.zeros((train_lstm_df_dict[1].shape[0], num_features))\n",
    "    print(train_predict_padded.shape)\n",
    "    train_predict_padded[lookback:, 0] = train_predict[:, 0]\n",
    "    test_predict_padded = np.zeros((test_lstm_df_dict[1].shape[0], num_features))\n",
    "    test_predict_padded[lookback:, 0] = test_predict[:, 0]\n",
    "    # inverse transform the predictions\n",
    "    train_predict_inversed = scaler_dict[1].inverse_transform(train_predict_padded)\n",
    "    test_predict_inversed = scaler_dict[1].inverse_transform(test_predict_padded)\n",
    "    # calculate root mean squared error\n",
    "    train_score = math.sqrt(mean_squared_error(train_lstm_df_dict[1].values[lookback:, 0], train_predict_inversed[lookback:, 0]))\n",
    "    print('Train Score: %.2f RMSE' % (train_score))\n",
    "    test_score = math.sqrt(mean_squared_error(test_lstm_df_dict[1].values[lookback:, 0], test_predict_inversed[lookback:, 0]))\n",
    "    print('Test Score: %.2f RMSE' % (test_score))\n",
    "    # shift train predictions for plotting\n",
    "    train_predict_plot = np.empty_like(train_lstm_df_dict[1].values)\n",
    "    train_predict_plot[:, :] = np.nan\n",
    "    train_predict_plot[lookback:len(train_predict_inversed)+lookback, :] = train_predict_inversed\n",
    "\n",
    "    # plot baseline and predictions\n",
    "    plt.plot(train_lstm_df_dict[1]['resale_price'], label=\"train\")\n",
    "    plt.plot(test_lstm_df_dict[1]['resale_price'], label=\"test\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 11ms/step\n",
      "(36, 1)\n",
      "21/21 [==============================] - 0s 9ms/step\n",
      "(39, 9)\n",
      "Train Score: 209290.70 RMSE\n",
      "Test Score: 247656.69 RMSE\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (39,9) into shape (36,9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ivankoh/personal/housing-data-engineering/modelling/lstm_keras.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivankoh/personal/housing-data-engineering/modelling/lstm_keras.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_lstm(model, train_lstm_dataset_dict[\u001b[39m1\u001b[39;49m], test_lstm_dataset_dict[\u001b[39m1\u001b[39;49m])\n",
      "\u001b[1;32m/Users/ivankoh/personal/housing-data-engineering/modelling/lstm_keras.ipynb Cell 12\u001b[0m in \u001b[0;36mevaluate_lstm\u001b[0;34m(model, train_set, test_set)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivankoh/personal/housing-data-engineering/modelling/lstm_keras.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_predict_plot \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(train_lstm_df_dict[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivankoh/personal/housing-data-engineering/modelling/lstm_keras.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_predict_plot[:, :] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ivankoh/personal/housing-data-engineering/modelling/lstm_keras.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m train_predict_plot[lookback:\u001b[39mlen\u001b[39;49m(train_predict_inversed)\u001b[39m+\u001b[39;49mlookback, :] \u001b[39m=\u001b[39m train_predict_inversed\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivankoh/personal/housing-data-engineering/modelling/lstm_keras.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# plot baseline and predictions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivankoh/personal/housing-data-engineering/modelling/lstm_keras.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(train_lstm_df_dict[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mresale_price\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (39,9) into shape (36,9)"
     ]
    }
   ],
   "source": [
    "evaluate_lstm(model, train_lstm_dataset_dict[1], test_lstm_dataset_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 9)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm_dataset_dict[1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm_dataset_dict[1][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 9)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm_df_dict[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataengineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
